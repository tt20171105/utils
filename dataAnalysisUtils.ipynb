{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, copy, functools\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df_train, df_test=None, object_cols=None, isdrop=True):\n",
    "    \"\"\"\n",
    "      This function can do one hot encoding of categorical features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_object_cols(df):\n",
    "        return list(df.select_dtypes(include=\"object\").columns)\n",
    "\n",
    "    df_train[\"train_test\"] = \"train\"\n",
    "    if df_test is not None:\n",
    "        df_test[\"train_test\"] = \"test\"\n",
    "        df_concat = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "        if object_cols is None: object_cols = list(set(get_object_cols(df_train) + get_object_cols(df_test)))\n",
    "    else:\n",
    "        df_concat = df_train.copy().reset_index(drop=True)\n",
    "        if object_cols is None: object_cols = list(set(get_object_cols(df_train)))\n",
    "\n",
    "    df_ohe     = pd.get_dummies(df_concat[object_cols], drop_first=True)\n",
    "    if isdrop:\n",
    "        df_ohe = pd.merge(df_concat.drop(object_cols, axis=1), df_ohe, left_index=True, right_index=True)\n",
    "    else:\n",
    "        df_ohe = pd.merge(df_concat, df_ohe, left_index=True, right_index=True)\n",
    "        \n",
    "    if df_test is not None:\n",
    "        df_ohe_train = df_ohe.query(\"train_test_train==1\").drop(\"train_test_train\", axis=1)\n",
    "        df_ohe_test  = df_ohe.query(\"train_test_train==0\").drop(\"train_test_train\", axis=1).reset_index(drop=True)\n",
    "        return df_ohe_train, df_ohe_test\n",
    "    else:\n",
    "        return df_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(df_train, df_test=None, numeric_cols=None):\n",
    "    \"\"\"\n",
    "      This function can do standardization of numerical features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_numeric_cols(df):\n",
    "        return list(df.select_dtypes(include=[\"int\",\"float\"]).columns)\n",
    "\n",
    "    if numeric_cols is None:\n",
    "        if df_test is not None:\n",
    "            numeric_cols = list(set(get_numeric_cols(df_train) + get_numeric_cols(df_test)))\n",
    "        else:\n",
    "            numeric_cols = list(set(get_numeric_cols(df_train)))\n",
    "    \n",
    "    mean   = df_train[numeric_cols].mean()\n",
    "    std    = df_train[numeric_cols].std()\n",
    "    df_train_std    = df_train.copy()\n",
    "    df_train_std[numeric_cols]    = df_train_std[numeric_cols].apply(lambda x: (x - mean[x.name]) / std[x.name])\n",
    "    if df_test is not None:\n",
    "        df_test_std = df_test.copy()\n",
    "        df_test_std[numeric_cols] = df_test_std[numeric_cols].apply( lambda x: (x - mean[x.name]) / std[x.name])\n",
    "        return df_train_std, df_test_std\n",
    "    else:\n",
    "        return df_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nan_feature(df, add_row_nan=True, add_is_nan=True):\n",
    "    \"\"\"\n",
    "      This function can add features about Nan.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_added_nan = df.copy()\n",
    "    print(\"The shape before adding features of Nan:\", df_added_nan.shape)\n",
    "    \n",
    "    if add_row_nan:\n",
    "        df_added_nan['number_of_nan'] = df_added_nan.isna().sum(axis=1).astype(np.int8)\n",
    "        \n",
    "    if add_is_nan:\n",
    "        for col in df_added_nan.columns:\n",
    "            if df_added_nan[col].isna().any():\n",
    "                df_added_nan[col + \"_nan\"] = np.where(df_added_nan[col].isna(), 1, 0)\n",
    "            \n",
    "    print(\"The shape after  adding features of Nan:\", df_added_nan.shape)\n",
    "    return df_added_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showStats(df):\n",
    "    \"\"\"\n",
    "      This function can show the statistics of features.\n",
    "      \n",
    "      Explaination of the result dataframe columns.\n",
    "        Feature name                      : カラム名\n",
    "        Unique values                     : カラムごとのユニーク数\n",
    "        Most frequent item                : 最も出現頻度の高い値\n",
    "        Freuquence of most frequent item  : 最も出現頻度の高い値の出現回数\n",
    "        Missing values(%)                 : 欠損損値の割合\n",
    "        Values in the biggest category(%) : 最も多いカテゴリの割合\n",
    "        Type                              : 型\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = []\n",
    "    for col in df.columns:\n",
    "        stats.append((col,\n",
    "                      df[col].nunique(),\n",
    "                      df[col].value_counts().index[0],\n",
    "                      df[col].value_counts().values[0],\n",
    "                      df[col].isnull().sum() * 100 / df.shape[0],\n",
    "                      df[col].value_counts(normalize=True, dropna=False).values[0] * 100,\n",
    "                      df[col].dtype))\n",
    "    df_stats = pd.DataFrame(stats, columns=['Feature name', 'Unique values', 'Most frequent item', 'Freuquence of most frequent item',\n",
    "                                            'Missing values(%)', 'Values in the biggest category(%)', 'Type'])\n",
    "    display(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showList(show_list, show_num=50, col=True):\n",
    "    \"\"\"\n",
    "      This function can show reshaped List.\n",
    "    \"\"\"\n",
    "    \n",
    "    reshaped_list = []\n",
    "    if show_num < len(show_list):\n",
    "        for i in range(0, len(show_list)+show_num, show_num):\n",
    "            if len(show_list) < i:\n",
    "                break\n",
    "            l = sorted(show_list)[i:i+show_num]\n",
    "            if len(l)==show_num:\n",
    "                reshaped_list.append(l)\n",
    "            else:\n",
    "                reshaped_list.append(l + [None]*(show_num-len(l)))\n",
    "    else:\n",
    "        reshaped_list = [sorted(show_list)]\n",
    "        \n",
    "    df_show_col = pd.DataFrame(reshaped_list)\n",
    "    if 0 < df_show_col.shape[1]:\n",
    "        display(df_show_col) if col else display(df_show_col.T)\n",
    "    else:\n",
    "        print(\"No features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureExtractionCorr(df, thrs=[0.99]):\n",
    "    \"\"\"\n",
    "      This function can do feature extraction by correration.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_features = {}\n",
    "    for thr in thrs:\n",
    "        dict_features[str(thr)] = []\n",
    "        \n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            for thr in thrs:\n",
    "                if thr < abs(corr_matrix.iloc[i, j]):\n",
    "                    dict_features[str(thr)].append(corr_matrix.columns[i])\n",
    "                    \n",
    "    for key, item in dict_features.items():\n",
    "        dict_features[key] = sorted(set(dict_features[key]))\n",
    "        print(key, \"The number of features is %s\" % len(dict_features[key]))\n",
    "        \n",
    "    del corr_matrix\n",
    "    return dict_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceMemUsage(df, verbose=False, y=[]):\n",
    "    \"\"\"\n",
    "      This function can reduce memory usage of DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    numerics  = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if (col in y) or (col_type not in numerics):\n",
    "            continue\n",
    "            \n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            if   c_min > np.iinfo(np.int8).min  and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)  \n",
    "        else:\n",
    "            if   c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                df[col] = df[col].astype(np.float16)\n",
    "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "                \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
